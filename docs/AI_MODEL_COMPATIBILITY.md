# AI Model Compatibility Guide

## ğŸ”§ Model-Specific Parameter Handling

Different OpenAI model series have different API parameter requirements. Our system automatically handles these differences.

---

## ğŸ“Š Model Parameter Matrix

| Model Series | max_tokens | max_completion_tokens | response_format | temperature (custom) |
|--------------|------------|----------------------|-----------------|---------------------|
| GPT-5        | âŒ         | âœ…                   | âœ…              | âŒ (fixed at 1)     |
| GPT-4o       | âœ…         | âŒ                   | âœ…              | âœ…                  |
| o1           | âŒ         | âœ…                   | âŒ              | âŒ (fixed at 1)     |
| GPT-4 Turbo  | âœ…         | âŒ                   | âœ…              | âœ…                  |
| GPT-4        | âœ…         | âŒ                   | âœ…              | âœ…                  |
| GPT-3.5      | âœ…         | âŒ                   | âœ…              | âœ…                  |

---

## ğŸ¯ Automatic Parameter Adaptation

Our API automatically detects the model and adjusts parameters:

### For GPT-5 and o1 Models
```typescript
{
  model: "gpt-5-mini",
  max_completion_tokens: 4000,  // New parameter name
  // No temperature (o1 only)
  // No response_format (o1 only)
}
```

### For GPT-4o, GPT-4, GPT-3.5 Models
```typescript
{
  model: "gpt-4o",
  max_tokens: 4000,  // Old parameter name
  temperature: 0.8,
  response_format: { type: "json_object" }
}
```

---

## ğŸ” Detection Logic

```typescript
// Detect new parameter models
const usesNewParams = aiModel.startsWith("gpt-5") || aiModel.startsWith("o1");

// o1 models have additional restrictions
const isO1Model = aiModel.startsWith("o1");
```

**GPT-5 Models:**
- Use `max_completion_tokens` âœ…
- Support `response_format` âœ…
- Temperature fixed at 1 (default) - custom values NOT supported âŒ

**o1 Models:**
- Use `max_completion_tokens` âœ…
- Do NOT support `response_format` âŒ
- Do NOT support `temperature` âŒ
- Have fixed temperature internally

**Legacy Models (GPT-4o, GPT-4, GPT-3.5):**
- Use `max_tokens` âœ…
- Support `response_format` âœ…
- Support `temperature` âœ…

---

## ğŸ’¡ Model-Specific Optimizations

### o1 Models (Reasoning-Focused)

**Special Handling:**
- Enhanced system message emphasizing JSON-only output
- No temperature control (uses internal optimization)
- No response_format enforcement
- Relies on prompt clarity for JSON output

**Best For:**
- Complex analytical questions
- Deep reasoning tasks
- Multi-step logic questions

**Not Ideal For:**
- Simple factual questions (overkill)
- High-volume generation (slow & expensive)
- When strict JSON format is critical (no format enforcement)

### GPT-5 Models (Latest)

**Capabilities:**
- Enhanced reasoning over GPT-4o
- Better multimodal understanding
- Improved contextual awareness
- Nuanced language handling (sarcasm, irony)

**Best For:**
- Creative, nuanced questions
- Complex sports analysis
- Latest features and capabilities

**Variants:**
- `gpt-5` - Full flagship model
- `gpt-5-mini` - Cost-effective, still powerful
- `gpt-5-nano` - Ultra-fast for simple tasks
- `gpt-5-chat` - Conversational optimization

### GPT-4o Models (Proven)

**Why Still Use:**
- Proven track record
- Reliable JSON formatting
- Fast responses
- Good cost/performance
- Well-tested with our prompts

**Recommended:**
- `gpt-4o` for production
- `gpt-4o-mini` for budget
- `gpt-4o-2024-11-20` for stability

---

## âš ï¸ Important Notes

### JSON Format Reliability

**High Reliability (response_format supported):**
- GPT-5 series
- GPT-4o series
- GPT-4 Turbo series
- GPT-4 series
- GPT-3.5 series

**Medium Reliability (no response_format):**
- o1 series - Relies on prompt instructions
- May occasionally include extra text
- Parsing logic handles this gracefully

### Temperature Settings

**Custom Temperature (0.8):**
- GPT-4o series âœ…
- GPT-4 Turbo series âœ…
- GPT-4 series âœ…
- GPT-3.5 series âœ…

**Fixed Temperature (Default = 1):**
- GPT-5 series - Only supports default temperature
- o1 series - Uses internal optimization
- More deterministic, less creative variation

### Token Limits

**GPT-5 and o1 Models: 16,000 tokens**
- These models use reasoning tokens internally
- Example: 4000 reasoning + 4000 output = 8000 total
- Higher limit ensures content is generated
- Don't hit limit during internal reasoning

**GPT-4o/GPT-4/GPT-3.5 Models: 4,000 tokens**
- Direct output generation
- No reasoning tokens
- 4000 is sufficient for 50 questions

**Important:** GPT-5/o1 models consume tokens differently:
- Reasoning tokens: Internal thinking (not visible)
- Completion tokens: Actual output
- Both count toward max_completion_tokens
- Need higher limits to avoid empty responses

---

## ğŸ§ª Testing Recommendations

### When Switching Models

1. **Generate Test Quiz**
   - Start with 5 questions
   - Check JSON format validity
   - Review question quality

2. **Verify Parsing**
   - Check if difficulty values are correct
   - Verify all required fields present
   - Test import process

3. **Compare Results**
   - Try same topic with different models
   - Compare quality and style
   - Check generation time

4. **Monitor Costs**
   - Track token usage
   - Compare per-question costs
   - Optimize model choice

---

## ğŸ“‹ Troubleshooting

### "Unsupported parameter" Error

**Cause:** Using old parameter with new model (or vice versa)

**Solution:** âœ… Already handled automatically by our code

### "Failed to parse generated quiz JSON"

**Possible with o1 models** (no response_format enforcement)

**Solution:**
- Check raw response in console
- Verify prompt clarity
- Consider using GPT-4o/GPT-5 instead
- Adjust prompt to emphasize JSON-only output

### Unexpected Results

**Different models have different styles:**
- GPT-5: More nuanced, creative
- GPT-4o: Reliable, consistent
- o1: Analytical, detailed
- GPT-3.5: Simpler, more direct

**Adjust expectations per model**

---

## âœ… Current Implementation

Our code handles ALL model differences automatically:

```typescript
// Detection
const usesNewParams = aiModel.startsWith("gpt-5") || aiModel.startsWith("o1");
const isO1Model = aiModel.startsWith("o1");

// Parameter adaptation
if (usesNewParams) {
  max_completion_tokens: 4000
} else {
  max_tokens: 4000
}

if (!isO1Model) {
  temperature: 0.8
}

if (!isO1Model) {
  response_format: { type: "json_object" }
}

// System message adaptation
if (isO1Model) {
  // Enhanced JSON-only instructions
}
```

**Result:** âœ… Works with ALL 23 models seamlessly!

---

## ğŸ‰ Model Compatibility Complete

You can now:
- âœ… Use any current OpenAI model
- âœ… Switch models without code changes
- âœ… Automatic parameter adaptation
- âœ… Graceful handling of model differences
- âœ… Ready for future models (GPT-6, etc.)

The system is fully compatible with all OpenAI models! ğŸš€

